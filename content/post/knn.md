---
title: "初次尝试K近邻算法"
date: 2020-10-25T09:28:00+08:00
lastmod: 2020-10-25T09:28:00+08:00
draft: true
tags: ["machine-learning", "knn"]
categories: ["machine-learning"]
author: mayf3

autoCollapseToc: true
contentCopyright: '<a href="https://github.com/mayf3/blog" rel="noopener" target="_blank">See origin</a>'
---

## 什么是k近邻算法

k近邻算法，是基本的分类与回归算法，不过这里也只讨论分类问题。
k近邻算法的思想很简单，对于每一个样本，会在训练数据里寻找与样本最接近的k个样本，根据分类决策的规则（一般是选k个样本中出现最多次数的），决定该样本的分类。

## k近邻算法有哪些参数
k近邻算法主要有三要素:
1. k值的选择
2. 距离度量
3. 分类决策规则

### k值的选择
k值的选择需要结合具体的应用数据，通常会选用较小的一个值，然后通过交叉验证法来选取最优的k值。
如果k值过小，其实就是减少偏差，但增大的方差，非常受数据干扰，比如极端情况k=1，找最近的点，则一旦数据噪声较多，则结果会大打折扣。
如果k值过大，其实就是减少方差，但增大了偏差，不太受数据的干扰，但正确性也会显著下降，比如极端情况k=N，则模型过于简单，完全忽略了数据中的有用信息，只会选择数量最多的分类。

### 距离度量
常见的距离有曼哈顿距离和欧式距离，一般采用欧式距离即可。

### 分类决策规则
最简单的分类决策规则，就是占比最大的数量，其实可以考虑根据距离来做一个加权平均，但没试验过，不知道效果如何。

## k近邻算法有哪些实现
k近邻算法最最简单的实现方法是线性扫描，对于每一个样本，都扫描一次排序找出前k小的。
另外一种常见的实现方法是采用kd树，在数据随机且维度不大的情况下，复杂度会优于线性扫描。

## k近邻算法的优缺点
k近邻算法的优点如下：
1. 实现简单
2. 不需要训练
3. 具有可解释性

k近邻算法的缺点如下：
1. 时间复杂度较高
2. 准确度不够高，只用到了数据的空间信息，对于大多数分类任务是不够用的。
